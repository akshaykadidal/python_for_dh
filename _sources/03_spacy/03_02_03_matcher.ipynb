{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "operational-landing",
   "metadata": {},
   "source": [
    "# The Matcher"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inappropriate-delicious",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b74fa1-49bb-4b34-b8ad-f73deb4786ca",
   "metadata": {},
   "source": [
    "SpaCy's built in Matcher component is another way a user can leverage spaCy to leverage linguistic annotations to find and extract structured data from unstructured text. Unlike the EntityRuler that we met in the previous section, the Matcher does not assign the identified patterns into an extension, such as `doc.ents`. Instead, the Matcher is designed to be leveraged outside of a spaCy pipeline. Also unlike the EntityRuler, the Matcher will not sit inside a spaCy pipeline, rather it will run over a spaCy Doc container."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719692d1-31ca-47b0-9ae4-ec27c1af71b2",
   "metadata": {},
   "source": [
    "## A Basic Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3593568a-a513-4bc5-9f1d-34cfc2d57798",
   "metadata": {},
   "source": [
    "To understand how this works, let's first look at a very basic example of the spaCy Matcher in practice. We will be working with the small spaCy English pipeline in this example, so let's go ahead and import the `Matcher` class and load up the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "naked-lighting",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "technical-anderson",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc118a4-b151-4bff-b087-a12bfce9fb31",
   "metadata": {},
   "source": [
    "Now that we have our pipeline loaded up into memory, let's go ahead and start working with the Matcher component. To load up the Matcher, we will use the Matcher class that we imported above. The Matcher class will take a single argument, the vocabulary of the nlp pipeline. We can access the nlp vocab by using the command `nlp.vocab`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1472fbbf-7d19-4981-a21d-4b1a62b8c41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher = Matcher(nlp.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05dbbad7-3fb3-4ce0-918c-b348d157311f",
   "metadata": {},
   "source": [
    "With our Matcher now loaded into memory, it is time to create a basic pattern. The pattern that we will want to find and extract from our texts is an email. As we will learn below, there are many token attributes you can leverage to create powerful rules-based pattern matchers in spaCy, rather like the EntityRuler we learned about in the previous section. One attribute we can look for is a Boolean (True or False) that looks to see if a token looks like an e-mail address. This attribute is `LIKE_EMAIL`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b100df95-1919-4299-8ba4-aa499907e935",
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = [\n",
    "    {\"LIKE_EMAIL\": True}\n",
    "            ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e0d79e-a49c-469f-89fa-3ef800d8a3cc",
   "metadata": {},
   "source": [
    "As with the EntityRuler, we can add these patterns into the Matcher. Unlike our EntityRuler, we will use the `.add` method, rather than `.add_patterns`. This method will take two arguments. First, the label that you want to assign to the matched token(s) and second the patterns themselves. Note that just like the EntityRuler, our patterns must be nested in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2d02bd4b-762d-4ca2-a828-9459cbeee111",
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher.add(\"EMAIL_ADDRESS\", [pattern])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f493f497-7e38-4602-8b50-bbe6e8a97865",
   "metadata": {},
   "source": [
    "With the Matcher now loaded in memory with patterns, we can run it over some text. Because the spaCy Matcher is a spaCy component, that text must first be converted into a spaCy Doc container. We can create that Doc container just as we normally would."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6b5d3cd7-dee8-41f8-a1d2-8f4030a021ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"This is an email address: wmattingly@aol.com\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc42e71-b29d-40bd-bbb5-6407e3f54a3f",
   "metadata": {},
   "source": [
    "We can then pass that Doc container to the Matcher as a single argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e1626592-0e3f-4a2d-85aa-95a7769c18e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = matcher(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696c1967-4f5c-496b-8a20-de0c33b17459",
   "metadata": {},
   "source": [
    "Let's go ahead and print off the `matches` object now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "synthetic-craps",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(16571425990740197027, 6, 7)]\n"
     ]
    }
   ],
   "source": [
    "print (matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "provincial-belarus",
   "metadata": {},
   "source": [
    "This may not be what you were expecting, but let's dive in and explore what is going on with this output. The output is a list with a single index. Each index will be a tuple. This tuple will consist of three parts: `lexeme`, `start token`, and `end token`. The lexeme is a numerical representation of our label in the nlp.vocab. We can access the label name by indexing the nlp.vocab at the lexeme index and then accessing the raw text with `.text`.\n",
    "\n",
    "Let's do each of these steps in turn. First, we will create an object called `first_match`. This will be our first hit with the Matcher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "mounted-morning",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16571425990740197027, 6, 7)\n"
     ]
    }
   ],
   "source": [
    "first_match = matches[0]\n",
    "print(first_match)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3913bf92-baf0-4ba9-9951-f58b04f43189",
   "metadata": {},
   "source": [
    "Next, we will grab lexeme, which is the first index in our `first_match`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "234c96e9-aa6f-43c9-acb2-95de82088e78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16571425990740197027\n"
     ]
    }
   ],
   "source": [
    "lexeme = first_match[0]\n",
    "print(lexeme)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443f76ca-ea10-433c-9c07-e30c11e8a7c6",
   "metadata": {},
   "source": [
    "Finally, we will print off the raw text of the label by indexing the nlp.vocab at that specific lexeme numerical value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d332564e-55ea-4a5c-ad7e-0aef41efc627",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EMAIL_ADDRESS\n"
     ]
    }
   ],
   "source": [
    "print(nlp.vocab[lexeme].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c86cb93d-05a8-40bb-bf45-87562a1fe72c",
   "metadata": {},
   "source": [
    "This is the basic idea behind the Matcher. It is a way of finding structured text using a pattern-based matching technique. As with the EntityRuler, there are a lot of token attributes we can leverage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gentle-nitrogen",
   "metadata": {},
   "source": [
    "## Attributes Taken by Matcher"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "egyptian-democracy",
   "metadata": {},
   "source": [
    "* ORTH - The exact verbatim of a token (str)\n",
    "* TEXT - The exact verbatim of a token (str)\n",
    "* LOWER - The lowercase form of the token text (str)\n",
    "* LENGTH - The length of the token text (int)\n",
    "* IS_ALPHA\n",
    "* IS_ASCII\n",
    "* IS_DIGIT\n",
    "* IS_LOWER\n",
    "* IS_UPPER\n",
    "* IS_TITLE\n",
    "* IS_PUNCT\n",
    "* IS_SPACE\n",
    "* IS_STOP\n",
    "* IS_SENT_START\n",
    "* LIKE_NUM\n",
    "* LIKE_URL\n",
    "* LIKE_EMAIL\n",
    "* SPACY\n",
    "* POS\n",
    "* TAG\n",
    "* MORPH\n",
    "* DEP\n",
    "* LEMMA\n",
    "* SHAPE\n",
    "* ENT_TYPE\n",
    "* _ - Custom extension attributes (Dict\\[str, Any\\])\n",
    "* OP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "synthetic-apache",
   "metadata": {},
   "source": [
    "## Applied Matcher"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6451087b-2834-49d6-b9d9-13c21cd82267",
   "metadata": {},
   "source": [
    "Let's now take a look at a practical application of the Matcher. Say, we had a text. In our case, it will be the following text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fifth-magnitude",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "Harry Potter was the main character in the book.\n",
    "Harry was a normal boy who discovered he was a wizard.\n",
    "Ultimately, Potter goes to Hogwarts.\n",
    "He is also known as the Boy who Lived.\n",
    "The Boy who Lived has an enemy named Voldemorte who is known as He who Must not be Named.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a040a6b9-1a6f-424e-8fd0-49eb9e8bda7a",
   "metadata": {},
   "source": [
    "Our goal in this exercise is to isolate all proper nouns and extract them. Ideally, we would like to extract proper nouns that are bigrams or trigrams and keep them intact.\n",
    "\n",
    "To do this, we will need to load up the spaCy small English pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "minute-sympathy",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "medieval-vintage",
   "metadata": {},
   "source": [
    "### Grabbing all Proper Nouns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75bfb3fe-d5ef-423a-b5e5-e5a21fdae672",
   "metadata": {},
   "source": [
    "Our initial pattern will be quite simple. We are simply going to grab all patterns where a single token has a part-of-speech (POS) is \"PROPN\", or proper noun."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cosmetic-combination",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "(3232560085755078826, 1, 2) Harry\n",
      "(3232560085755078826, 2, 3) Potter\n",
      "(3232560085755078826, 12, 13) Harry\n",
      "(3232560085755078826, 27, 28) Potter\n",
      "(3232560085755078826, 30, 31) Hogwarts\n",
      "(3232560085755078826, 52, 53) Voldemorte\n"
     ]
    }
   ],
   "source": [
    "matcher = Matcher(nlp.vocab)\n",
    "pattern = [{\"POS\": \"PROPN\"}]\n",
    "matcher.add(\"PROPER_NOUNS\", [pattern])\n",
    "doc = nlp(text)\n",
    "matches = matcher(doc)\n",
    "print (len(matches))\n",
    "for match in matches[:10]:\n",
    "    print (match, doc[match[1]:match[2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "transparent-hughes",
   "metadata": {},
   "source": [
    "### Improving it with Multi-Word Tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d336efe-2469-4b8d-b529-4bb8a2cc00e5",
   "metadata": {},
   "source": [
    "While the above output is good, it does not capture multi-word-tokens, such as `Harry Potter`. Ideally, we would like to find and extract these instances. We can do this by passing in a second argument in our pattern after the POS: `OP` and set it to `+`. This will look for any sequence of proper nouns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "medical-method",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "(3232560085755078826, 1, 2) Harry\n",
      "(3232560085755078826, 1, 3) Harry Potter\n",
      "(3232560085755078826, 2, 3) Potter\n",
      "(3232560085755078826, 12, 13) Harry\n",
      "(3232560085755078826, 27, 28) Potter\n",
      "(3232560085755078826, 30, 31) Hogwarts\n",
      "(3232560085755078826, 52, 53) Voldemorte\n"
     ]
    }
   ],
   "source": [
    "matcher = Matcher(nlp.vocab)\n",
    "pattern = [{\"POS\": \"PROPN\", \"OP\": \"+\"}]\n",
    "matcher.add(\"PROPER_NOUNS\", [pattern])\n",
    "doc = nlp(text)\n",
    "matches = matcher(doc)\n",
    "print (len(matches))\n",
    "for match in matches[:10]:\n",
    "    print (match, doc[match[1]:match[2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mathematical-express",
   "metadata": {},
   "source": [
    "### Greedy Keyword Argument"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be796d0d-4829-486d-af7e-4441a9a0cf36",
   "metadata": {},
   "source": [
    "As we can see, this has worked pretty well, but we have a key issue. `Harry` is grabbed as is `Harry Potter` as is `Potter`. These are all three instances of a single multi-word token: `Harry Potter`.\n",
    "\n",
    "The Matcher can also take another keyword argument when we add patterns: `greedy`. This will take one of two possible arguments: `FIRST` or `LONGEST`. They each define how spaCy will function when it encounters two matches that have overlapping spans. `FIRST` will extract the first hit with overlapping spans, while `LONGEST` will extract only the longest. Let's consider the same example above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "located-employer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "(3232560085755078826, 1, 3) Harry Potter\n",
      "(3232560085755078826, 12, 13) Harry\n",
      "(3232560085755078826, 27, 28) Potter\n",
      "(3232560085755078826, 30, 31) Hogwarts\n",
      "(3232560085755078826, 52, 53) Voldemorte\n"
     ]
    }
   ],
   "source": [
    "matcher = Matcher(nlp.vocab)\n",
    "pattern = [{\"POS\": \"PROPN\", \"OP\": \"+\"}]\n",
    "matcher.add(\"PROPER_NOUNS\", [pattern], greedy='LONGEST')\n",
    "doc = nlp(text)\n",
    "matches = matcher(doc)\n",
    "print (len(matches))\n",
    "for match in matches[:10]:\n",
    "    print (match, doc[match[1]:match[2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aware-stupid",
   "metadata": {},
   "source": [
    "### Adding in Sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7932f78c-b150-46af-b6b1-6db26202360c",
   "metadata": {},
   "source": [
    "Let's say I not only wanted to extract these type of instances, but I wanted to look for more robust patterns. What if I wanted to look for places in the text where Harry Potter is followed by a verb of action (not a verb of to be). We can do this by passing a third component to our pattern: `POS` is `VERB`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "individual-timber",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "(3232560085755078826, 27, 29) Potter goes\n"
     ]
    }
   ],
   "source": [
    "matcher = Matcher(nlp.vocab)\n",
    "pattern = [{\"POS\": \"PROPN\", \"OP\": \"+\"}, {\"POS\": \"VERB\"}]\n",
    "matcher.add(\"PROPER_NOUNS\", [pattern], greedy='LONGEST')\n",
    "doc = nlp(text)\n",
    "matches = matcher(doc)\n",
    "matches.sort(key = lambda x: x[1])\n",
    "print (len(matches))\n",
    "for match in matches[:10]:\n",
    "    print (match, doc[match[1]:match[2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2890319-3f0d-418a-a778-6683c9026ec3",
   "metadata": {},
   "source": [
    "And just like this, we were able to extract an instance of Harry Potter's name followed by a verb of action. In our case, `goes`. While this example is quite simple, the Matcher allows for robust pattern matching with spaCy containers. The patterns that we looked at here can also be used and applied to the EntityRuler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee59a265-aacf-44cd-bdff-f42e9bfe3ed0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
