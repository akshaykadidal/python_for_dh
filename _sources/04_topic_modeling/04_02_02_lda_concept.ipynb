{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "613bae22-5138-407f-85c3-89e00a8d7ab3",
   "metadata": {},
   "source": [
    "# <center>What is LDA Topic Modeling?</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c16519-2e6d-47b9-a485-6ad0a30af9e0",
   "metadata": {},
   "source": [
    "Latent Dirichlet Allocation (LDA) topic modeling originated in population genomics in 2000 as a way to understand larger patterns in genomics data. In 2003, it was applied to machine learning, specifically texts to solve the problem of topic discovery. It leverages statistics to identify topics across a distributed set of data.\n",
    "\n",
    "In order to engage in LDA Topic Modeling, one must clean a corpus significantly. Common steps that we will cover in this chapter are:\n",
    "\n",
    "- removing of stopwords\n",
    "- lemmatization\n",
    "- removing of punctuation\n",
    "\n",
    "In addition to cleaning the corpus, one must reduce the documents down to a TF-IDF vector which we will learn how to do via the Scikit-Learn library in Python. Finally, one must pass all TF-IDF vectors to the LDA Topic Model along with a specified amount of topics to identify across the entire corpus. Assigning this number can be quite challenging and will require a series of trial-and-error passes. As we will see in the next chapter, Transformer-Based Topic Modeling, a lot of these issues, from cleaning a corpus to guessing the quantity of topics in the corpus, are eliminated with the use of transformers and more recent machine learning methods and algorithms.\n",
    "\n",
    "Once an LDA topic model is trained, it can then be used to interrogate the corpus collectively. One can explore the topics identified and by reading the documents that the model placed in the same category. Depending on the corpus, certain topics can sometimes be difficult to understand. At this stage, the topics do not have actual names, rather they are purely numerical.\n",
    "\n",
    "If we want to assign labels to each topic, we have two different options available to us. First, a content expert can look at the results and intuitively assign a label that makes the most sense given the corpus. Another approach is through automation. A researcher can find the most common and unique words that appear in a topic and automatically assign them to the label name.\n",
    "\n",
    "By the end of this chapter, we will create a topic model, but before we can do that, we must have a firm grasp of each step in the cleaning process. That means that we must first have a basic understanding of TF-IDF, how it works, and why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5badc7-a917-4214-b8b5-6da4392bd981",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
