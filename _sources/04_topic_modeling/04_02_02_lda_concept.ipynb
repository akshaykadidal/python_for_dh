{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "613bae22-5138-407f-85c3-89e00a8d7ab3",
   "metadata": {},
   "source": [
    "# LDA Topic Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c16519-2e6d-47b9-a485-6ad0a30af9e0",
   "metadata": {},
   "source": [
    "A common way to perform topic modeling in the digital humanities is via Latent Dirichlet Allocation (LDA) topic modeling. This method originated in [population genomics in 2000](https://academic.oup.com/genetics/article/155/2/945/6048111?login=false) as a way to understand larger patterns in genomics data. [In 2003](https://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf), it was applied to machine learning, specifically texts to solve the problem of topic discovery. It leverages statistics to identify topics across a distributed set of data.\n",
    "\n",
    "\n",
    "## Process of Topic Modeling\n",
    "\n",
    "Here, let's consider a simple example. We will work with the following document from the TRC Volume 7 dataset.\n",
    "\n",
    "`An ANCYL member who was shot and severely injured by SAP members at Lephoi, Bethulie, Orange Free State (OFS) on 17 April 1991. Police opened fire on a gathering at an ANC supporter's house following a dispute between two neighbours, one of whom was linked to the ANC and the other to the SAP and a councillor.`\n",
    "\n",
    "In order to engage in LDA Topic Modeling, one must clean a corpus significantly. Common steps that we will cover in this chapter are:\n",
    "\n",
    "- removing of stopwords\n",
    "- lemmatization (optional)\n",
    "- removing of punctuation\n",
    "\n",
    "Stopwords consist of words common across all texts. The official list from the `NLTK` librar is as follows:\n",
    "\n",
    "```\n",
    "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
    "\n",
    "```\n",
    "\n",
    "After we perform these steps (with the exception of lemmatization), we have the following result:\n",
    "\n",
    "```\n",
    "['ancyl', 'member', 'shot', 'severely', 'injured', 'sap', 'members', 'lephoi', 'bethulie', 'orange', 'free', 'state', 'ofs', '17', 'april', '1991', 'police', 'opened', 'fire', 'gathering', 'anc', 'supporters', 'house', 'following', 'dispute', 'two', 'neighbours', 'one', 'linked', 'anc', 'sap', 'councillor']\n",
    "```\n",
    "\n",
    "In addition to cleaning the corpus, one must also reduce all words to unique numbers. These numbers have now relevance to the word, rather they are a single integer. In this approach, a researcher creates a bag-of-words (BOW) dictionary. This dictionary is a collection of integers and their corresponding word. Each text is then reduced to a sequence of numbers. Once we perform this step, our documents now look like this:\n",
    "\n",
    "```\n",
    "[(0, 1), (1, 1), (2, 2), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1), (20, 1), (21, 1), (22, 1), (23, 1), (24, 2), (25, 1), (26, 1), (27, 1), (28, 1), (29, 1)]\n",
    "```\n",
    "\n",
    "This sequence of numbers refers to the presence of a word in this document. If we were to print off each individual word from our entire dictionary, we would see that it looks like this:\n",
    "\n",
    "```\n",
    "0\t17\n",
    "1\t1991\n",
    "2\tanc\n",
    "3\tancyl\n",
    "4\tapril\n",
    "5\tbethulie\n",
    "6\tcouncillor\n",
    "7\tdispute\n",
    "8\tfire\n",
    "9\tfollowing\n",
    "10\tfree\n",
    "11\tgathering\n",
    "12\thouse\n",
    "13\tinjured\n",
    "14\tlephoi\n",
    "15\tlinked\n",
    "16\tmember\n",
    "17\tmembers\n",
    "18\tneighbours\n",
    "19\tofs\n",
    "20\tone\n",
    "21\topened\n",
    "22\torange\n",
    "23\tpolice\n",
    "24\tsap\n",
    "25\tseverely\n",
    "26\tshot\n",
    "27\tstate\n",
    "28\tsupporters\n",
    "29\ttwo\n",
    "\n",
    "```\n",
    "\n",
    "As we will see when we explore Top2Vec later in this chapter, a major drawback to this approach is that we do not retain any information about the syntax or semantics of the language in this approach. Rather, we just know if a word appears or not. Language is, however, far more complex than simply the presence or lack of presence of words in order to reach meaning. More importantly, word order dramatically affects meaning of individual words due to context. Traditional LDA topic modeling does not offer solutions to these problems.\n",
    "\n",
    "## Knowing the Total Number of Topics\n",
    "\n",
    "Another issue with LDA Topic Modeling is that one must know a specified amount of topics to identify across the entire corpus. Assigning this number can be quite challenging and will require a series of trial-and-error passes. As we will see in the next chapter, Transformer-Based Topic Modeling, a lot of these issues, from cleaning a corpus to guessing the quantity of topics in the corpus, are eliminated with the use of transformers and more recent machine learning methods and algorithms.\n",
    "\n",
    "## Applying a Topic Model\n",
    "\n",
    "Once an LDA topic model is trained, it can then be used to interrogate the corpus collectively. One can explore the topics identified and by reading the documents that the model placed in the same category. Depending on the corpus, certain topics can sometimes be difficult to understand. At this stage, the topics do not have actual names, rather they are purely numerical.\n",
    "\n",
    "If we want to assign labels to each topic, we have two different options available to us. First, a content expert can look at the results and intuitively assign a label that makes the most sense given the corpus. Another approach is through automation. A researcher can find the most common and unique words that appear in a topic and automatically assign them to the label name.\n",
    "\n",
    "By the end of this chapter, we will create a topic model via Top2Vec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5badc7-a917-4214-b8b5-6da4392bd981",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
