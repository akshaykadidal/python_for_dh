{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e8e3787-642e-4497-9dda-c12775b481ea",
   "metadata": {},
   "source": [
    "# Top2Vec in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0614e5ba-bd50-4767-9074-4e142772c528",
   "metadata": {},
   "source": [
    "Now that we understand the primary concepts behind leveraging transformers and sentence embeddings to perform topic modeling, let's examine a key library and making this entire workflow simplified with just a single line of Python. That library is Top2Vec.\n",
    "\n",
    "Unlike a traditional topic modeling approach or even the manual approach with transformers that we saw in the previous section, Top2Vec will embed not only all the sentences, but all the words used in your corpus. In addition to this, it will also embed the topics themselves. This means that words, documents, and topics all occupy the same higher-dimensional space. On the surface, this may not seem that important, but it allows us to understand the relationship between three interconnected pieces of the process in topic modeling mathematically. This means, for example, Top2Vec let's us know the mathematical similarity between a given word and a document or a topic in general. Understanding these relationships lets us understand broader patterns in our topics that may otherwise be missed.\n",
    "\n",
    "That said, Top2Vec does have several drawbacks. First, it will produce a high number of outliers. These are documents that do not fit neatly into any one topic. Top2Vec has a rather unique way of handling outliers, however. It generate an embedding for the topics 0 and above and then assign outliers to their nearest topic embedding. This means that outliers may be quite pronounced for certain topics. For some workflows, this may not be an issue. Second, Top2Vec can produce results that are occasionally difficult to understand. Certain topics may not seem to have any coherency. This is common in all approaches to topic modeling, however.\n",
    "\n",
    "In order to model our data, we once again need to load it. Again, we will use pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3200964-ab21-40d9-986a-c5ef8a8cfbf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6455f89c-03ff-4f10-aedc-405c33a52b44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>names</th>\n",
       "      <th>descriptions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AARON, Thabo Simon</td>\n",
       "      <td>An ANCYL member who was shot and severely inju...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ABBOTT, Montaigne</td>\n",
       "      <td>A member of the SADF who was severely injured ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ABDUL WAHAB, Zakier</td>\n",
       "      <td>A member of QIBLA who disappeared in September...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ABRAHAM, Nzaliseko Christopher</td>\n",
       "      <td>A COSAS supporter who was kicked and beaten wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ABRAHAMS, Achmat Fardiel</td>\n",
       "      <td>Was shot and blinded in one eye by members of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21742</th>\n",
       "      <td>ZWENI, Ernest</td>\n",
       "      <td>One of two South African Police (SAP) members ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21743</th>\n",
       "      <td>ZWENI, Lebuti</td>\n",
       "      <td>An ANC supporter who was shot dead by a named ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21744</th>\n",
       "      <td>ZWENI, Louis</td>\n",
       "      <td>Was shot dead in Tokoza, Transvaal, on 22 May ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21745</th>\n",
       "      <td>ZWENI, Mpantesa William</td>\n",
       "      <td>His home was lost in an arson attack by Witdoe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21746</th>\n",
       "      <td>ZWENI, Xolile Milton</td>\n",
       "      <td>A Transkei Defence Force (TDF) soldier who was...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21747 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                names  \\\n",
       "0                  AARON, Thabo Simon   \n",
       "1                   ABBOTT, Montaigne   \n",
       "2                 ABDUL WAHAB, Zakier   \n",
       "3      ABRAHAM, Nzaliseko Christopher   \n",
       "4            ABRAHAMS, Achmat Fardiel   \n",
       "...                               ...   \n",
       "21742                   ZWENI, Ernest   \n",
       "21743                   ZWENI, Lebuti   \n",
       "21744                    ZWENI, Louis   \n",
       "21745         ZWENI, Mpantesa William   \n",
       "21746            ZWENI, Xolile Milton   \n",
       "\n",
       "                                            descriptions  \n",
       "0      An ANCYL member who was shot and severely inju...  \n",
       "1      A member of the SADF who was severely injured ...  \n",
       "2      A member of QIBLA who disappeared in September...  \n",
       "3      A COSAS supporter who was kicked and beaten wi...  \n",
       "4      Was shot and blinded in one eye by members of ...  \n",
       "...                                                  ...  \n",
       "21742  One of two South African Police (SAP) members ...  \n",
       "21743  An ANC supporter who was shot dead by a named ...  \n",
       "21744  Was shot dead in Tokoza, Transvaal, on 22 May ...  \n",
       "21745  His home was lost in an arson attack by Witdoe...  \n",
       "21746  A Transkei Defence Force (TDF) soldier who was...  \n",
       "\n",
       "[21747 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_json(\"https://raw.githubusercontent.com/wjbmattingly/bap_sent_embedding/main/data/vol7.json\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419226f4-91f6-4527-9bf8-0a636f78338b",
   "metadata": {},
   "source": [
    "Once again, we will convert all our descriptions into a single list of documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ffdc3d9a-e72b-4ad3-8b31-61b67747bc89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"An ANCYL member who was shot and severely injured by SAP members at Lephoi, Bethulie, Orange Free State (OFS) on 17 April 1991. Police opened fire on a gathering at an ANC supporter's house following a dispute between two neighbours, one of whom was linked to the ANC and the other to the SAP and a councillor.\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = df.descriptions.tolist()\n",
    "docs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced4cada-75b4-40e1-a7d9-f1d5b314e2b2",
   "metadata": {},
   "source": [
    "## Creating a Top2Vec Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51bd8207-bc66-4b51-80f5-15f2aa52ab4b",
   "metadata": {},
   "source": [
    "Now that we have our data, we can get started with Top2Vec. First, you will need to install the library onto your system. In order to do this, you can use `pip` with the following command:\n",
    "\n",
    "```\n",
    "pip install top2vec\n",
    "```\n",
    "\n",
    "This will install Top2Vec as well as all its dependencies. Once it is installed, you will be able to import the Top2Vec class with the following line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b076124-eaae-4677-aa99-2c5219a87590",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wjbmattingly/anaconda3/envs/python-textbook/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from top2vec import Top2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40afd10c-b872-44f3-84dd-d29ebf260704",
   "metadata": {},
   "source": [
    "Once we have imported the Top2Vec class, we can create our Top2Vec topic model with just one line of Python. We can pass many key word arguments to our Top2Vec class, but the result from the default settings is usually quite good. It may take several minutes or even hours to run the code below, depending on your system requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2af545e3-474c-4be3-8910-eec70cb6e8a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-13 13:58:09,857 - top2vec - INFO - Pre-processing documents for training\n",
      "2022-11-13 13:58:11,379 - top2vec - INFO - Creating joint document/word embedding\n",
      "2022-11-13 13:58:53,163 - top2vec - INFO - Creating lower dimension embedding of documents\n",
      "2022-11-13 13:59:16,683 - top2vec - INFO - Finding dense areas of documents\n",
      "2022-11-13 13:59:18,018 - top2vec - INFO - Finding topics\n"
     ]
    }
   ],
   "source": [
    "model = Top2Vec(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc3b017-2ad8-4ec5-a0d6-8582c80f09b4",
   "metadata": {},
   "source": [
    "## Analyzing our Topic Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f40dee-0547-48bd-bc8e-b34228ba1b9f",
   "metadata": {},
   "source": [
    "Once our Top2Vec topic model has finished training, we can then analyze the results. A good first step is to analyze the topic sizes. We can do so with the model `.get_topic_sizes()`. This method will return a list of two lists: the size of each topic and its number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8e185a1-4f19-4102-bd6a-099dc165e850",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_sizes, topic_nums = model.get_topic_sizes()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37decb64-5fd3-4d9e-8005-af922ab0730f",
   "metadata": {},
   "source": [
    "We can better understand these results by zipping the two lists together and iterating over the first five topics to see how large they are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2aec8764-6232-4ed0-8b07-6f769e1289a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic Num 0 has 763 documents.\n",
      "Topic Num 1 has 346 documents.\n",
      "Topic Num 2 has 247 documents.\n",
      "Topic Num 3 has 232 documents.\n",
      "Topic Num 4 has 231 documents.\n"
     ]
    }
   ],
   "source": [
    "for topic_size, topic_num in zip(topic_sizes[:5], topic_nums[:5]):\n",
    "    print(f\"Topic Num {topic_num} has {topic_size} documents.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c387865-f5dd-4bb5-98ad-73cc6725e639",
   "metadata": {},
   "source": [
    "Notice that the higher number the topic number, the smaller the quantity of documents. This is because Top2Vec organizes the topics by size. Remember that `Topic 0` will be a collection of outliers.\n",
    "\n",
    "We can use the `.get_topics()` method to extract information about our topics from the model. This will take one argument, the quantity of topics you want to examine. It will return 3 items: a list of words for each topic, a list of word scores for each list, and a list of the topic numbers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4eb06ee5-a558-44f8-af6c-111736446b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_words, word_scores, topics = model.get_topics(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4746eea3-3572-488a-ac9d-c339de777029",
   "metadata": {},
   "source": [
    "Let's now examine the data for topic 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f120463c-bb70-4595-a780-f8f0c5738f71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1\n",
      "matlala 0.8607659\n",
      "bk 0.8543051\n",
      "gamatlala 0.8241883\n",
      "proposed 0.794874\n",
      "lebowa 0.74230933\n",
      "resisted 0.67116654\n",
      "independence 0.65576303\n",
      "africa 0.6098439\n",
      "chief 0.49963036\n",
      "she 0.39457357\n",
      "because 0.3908882\n",
      "goederede 0.36000514\n",
      "february 0.3470583\n",
      "mahlangu 0.34650403\n",
      "home 0.34591654\n",
      "down 0.34060025\n",
      "south 0.33364677\n",
      "dennilton 0.3321176\n",
      "burnt 0.32735756\n",
      "would 0.3133811\n",
      "supported 0.31247702\n",
      "incorporation 0.3080003\n",
      "stolen 0.28846616\n",
      "from 0.28013867\n",
      "had 0.27430868\n",
      "it 0.2732216\n",
      "lost 0.2716071\n",
      "kwandebele 0.266503\n",
      "congress 0.26175743\n",
      "allegedly 0.25916496\n",
      "supporters 0.25534543\n",
      "village 0.25463325\n",
      "her 0.25239387\n",
      "mr 0.2508838\n",
      "homeland 0.24748528\n",
      "spite 0.24678819\n",
      "about 0.24613273\n",
      "maboloko 0.24536304\n",
      "resulted 0.24119411\n",
      "into 0.24059604\n",
      "opposed 0.23594634\n",
      "pietersburg 0.23558588\n",
      "alleged 0.22179778\n",
      "possessions 0.21927707\n",
      "many 0.21916199\n",
      "house 0.21861681\n",
      "imbokodo 0.21361831\n",
      "leeuwfontein 0.21190558\n",
      "tribal 0.21142614\n",
      "zeerust 0.21082786\n"
     ]
    }
   ],
   "source": [
    "for words, scores, num in zip(topic_words[1:], word_scores[1:], topics[1:]):\n",
    "    print(f\"Topic {num}\")\n",
    "    for word, score in zip(words, scores):\n",
    "        print(word, score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcfa8335-2b24-438f-ae50-3533a6fe4d9e",
   "metadata": {},
   "source": [
    "This gives us a word list of 50 words most closely associated with `Topic 1`. Each word has a score. Remember, words, documents, and topics all occupy the same higher-dimensional space. This is an example of this advantage in practice. We are able to calculate the degree to which a word is relevant to a topic because of its proximity to the center of a topic. The higher the score, the closer a word is to a topic's center and, therefore, in theory more closely represents that cluster.\n",
    "\n",
    "We can also use the `search_documents_by_topic()` method to learn about the documents that reside within a topic. This method takes two arguments: `topic_num` (the number of the topic you wish to examine) and `num_docs` (the number of documents you want to return). This method will return three things: the documents, the scores, or the degree to which they represent the topic, and their unique ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9be21372-ee7b-4c33-8a3b-957b80083df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents, document_scores, document_ids = model.search_documents_by_topic(topic_num=0, num_docs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0949c7-ce2c-4dd2-ba0a-bb1024def72b",
   "metadata": {},
   "source": [
    "Now that we have our data, we can iterate them all simultaneously with the example provided in the Top2Vec README on GitHub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "da86b28b-7c9f-4718-88e2-c9ffba43dcea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document: 15060, Score: 0.9702943563461304\n",
      "-----------\n",
      "An ANC supporter who had her house burnt down by IFP supporters in Sonkombo, Ndwedwe, KwaZulu, near Durban, on 16 March 1994. See Sonkombo arson attacks. \n",
      "-----------\n",
      "\n",
      "Document: 5686, Score: 0.965286374092102\n",
      "-----------\n",
      "She lost her house in Sonkombo, Ndwedwe, KwaZulu, near Durban, in an arson attack by IFP supporters on 16 March 1994. See Sonkombo arson attacks.\n",
      "-----------\n",
      "\n",
      "Document: 21471, Score: 0.9650641083717346\n",
      "-----------\n",
      "An ANC supporter whose house was burnt down by IFP supporters in Sonkombo, Ndwedwe, KwaZulu, near Durban, on 16 March 1994. See Sonkombo arson attacks. \n",
      "-----------\n",
      "\n",
      "Document: 2957, Score: 0.9647031426429749\n",
      "-----------\n",
      "His home was burnt down by IFP supporters on 16 March 1994 at Sonkombo, Ndwedwe, KwaZulu, near Durban, in intense political conflict in the area. See Sonkombo arson attacks.\n",
      "-----------\n",
      "\n",
      "Document: 714, Score: 0.9634069204330444\n",
      "-----------\n",
      "An ANC supporter who had her home burnt down by IFP supporters at Sonkombo, Ndwedwe, KwaZulu, near Durban, on 16 March 1994. See Sonkombo arson attacks.\n",
      "-----------\n",
      "\n",
      "Document: 9855, Score: 0.9628040194511414\n",
      "-----------\n",
      "He had his house burnt down by IFP supporters in Sonkombo, Ndwedwe, KwaZulu, near Durban, on 16 March 1994. See Sonkombo arson attacks. \n",
      "-----------\n",
      "\n",
      "Document: 759, Score: 0.9617692232131958\n",
      "-----------\n",
      "Had her home burnt down by IFP supporters at Sonkombo, Ndwedwe, KwaZulu, near Durban, on 16 March 1994. See Sonkombo arson attacks.\n",
      "-----------\n",
      "\n",
      "Document: 519, Score: 0.9617335796356201\n",
      "-----------\n",
      "An ANC supporter who had her house burnt down by IFP supporters on 16 March 1994 at Sonkombo, Ndwedwe, KwaZulu, near Durban, in intense political conflict in the area. See Sonkombo arson attacks.\n",
      "-----------\n",
      "\n",
      "Document: 3424, Score: 0.9609926342964172\n",
      "-----------\n",
      "An ANC supporter whose home was burnt down by IFP supporters on 20 March 1994 at Sonkombo, Ndwedwe, KwaZulu, near Durban. See Sonkombo arson attacks.\n",
      "-----------\n",
      "\n",
      "Document: 12659, Score: 0.9603915810585022\n",
      "-----------\n",
      "An ANC supporter who had her house burnt down by IFP supporters in Sonkombo, Ndwedwe, KwaZulu, near Durban, on 20 March 1994. See Sonkombo arson attacks. \n",
      "-----------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for doc, score, doc_id in zip(documents, document_scores, document_ids):\n",
    "    print(f\"Document: {doc_id}, Score: {score}\")\n",
    "    print(\"-----------\")\n",
    "    print(doc)\n",
    "    print(\"-----------\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc94b1ce-f745-4b97-9b5c-e3c41149cb7a",
   "metadata": {},
   "source": [
    "These results are good. These are all individuals who have near identical descriptions in the TRC Volume 7 Final Report. In other words, Top2Vec has clearly isolated documents of identical or overlapping similarity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31cd1bec-d88f-4499-98fb-c04b6622416b",
   "metadata": {},
   "source": [
    "## Working with Bigrams and Trigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3bca702-851f-4919-baee-b40021c62c09",
   "metadata": {},
   "source": [
    "Top2Vec also wraps around the Gensim library and allows users to automatically create bigrams and trigrams. We can initiate this process by setting the keyword argument `ngram_vocba` to `True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "89a9c3ec-c7e7-451a-afd3-9f75ac7037c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-13 13:59:18,293 - top2vec - INFO - Pre-processing documents for training\n",
      "2022-11-13 13:59:19,889 - top2vec - INFO - Creating joint document/word embedding\n",
      "2022-11-13 14:00:07,661 - top2vec - INFO - Creating lower dimension embedding of documents\n",
      "2022-11-13 14:00:19,175 - top2vec - INFO - Finding dense areas of documents\n",
      "2022-11-13 14:00:19,748 - top2vec - INFO - Finding topics\n"
     ]
    }
   ],
   "source": [
    "model_ngram = Top2Vec(docs, ngram_vocab=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b51de0-fb95-4a76-bb56-8e02750b5d16",
   "metadata": {},
   "source": [
    "Now that we have our `model_ngram` trained, let's explore topic 1 once again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "96db6cc3-f675-4c61-9b82-275684b74f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_sizes_ngram, topic_nums_ngram = model_ngram.get_topic_sizes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "de65722e-9e6a-4a35-8d16-1e6a4ef6a406",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1\n",
      "island 0.7406236\n",
      "imprisonment 0.7383562\n",
      "years 0.7350636\n",
      "robben 0.73397404\n",
      "half years 0.7247492\n",
      "years imprisonment 0.7206887\n",
      "sentenced 0.6986825\n",
      "served 0.695978\n",
      "arrested 0.66353947\n",
      "charged 0.66336066\n",
      "banned 0.66275764\n",
      "imprisoned 0.6580816\n",
      "five years 0.6498868\n",
      "suspended sentence 0.6363463\n",
      "prison sentence 0.6340719\n",
      "trial 0.6326254\n",
      "sentence 0.62952983\n",
      "activist 0.6273678\n",
      "months 0.62542427\n",
      "detained 0.6249351\n",
      "prison 0.6236554\n",
      "until 0.6223862\n",
      "poqo 0.6147149\n",
      "custody 0.6120131\n",
      "year sentence 0.6114016\n",
      "held 0.6040434\n",
      "worcester 0.60011864\n",
      "pac 0.5935757\n",
      "released 0.5893775\n",
      "poqo activities 0.58773917\n",
      "charges 0.5808439\n",
      "zwelethemba worcester 0.57607645\n",
      "zweletemba worcester 0.575092\n",
      "arrest 0.5745207\n",
      "robben island 0.571052\n",
      "under 0.56998795\n",
      "worcester cape 0.56770444\n",
      "detention 0.5670366\n",
      "with public 0.56048626\n",
      "ashton 0.55939454\n",
      "later acquitted 0.5478358\n",
      "tortured 0.5445701\n",
      "charged with 0.54265773\n",
      "act 0.5392955\n",
      "interrogation 0.5369543\n",
      "without 0.53288996\n",
      "paarl 0.53190106\n",
      "public 0.5305038\n",
      "prison paarl 0.5296649\n",
      "year prison 0.5292457\n"
     ]
    }
   ],
   "source": [
    "topic_words_ngram, word_scores_ngram, topics_ngram = model_ngram.get_topics(2)\n",
    "for words, scores, num in zip(topic_words_ngram[1:], word_scores_ngram[1:], topics_ngram[1:]):\n",
    "    print(f\"Topic {num}\")\n",
    "    for word, score in zip(words, scores):\n",
    "        print(word, score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c2b71e-efb7-408a-978d-fe5670fbd9aa",
   "metadata": {},
   "source": [
    "It is important to note that this Topic 1 is entirely different from the Topic 1 of our first model as both the initialization of the process is random and the data used for embedding documents is also altered (because we are accepting bigrams). Our data does reflect a deeper representation of our corpus. `Prison sentence`, for example, is a collective word that has a distinct meaning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02061c9-cf07-4141-885e-153fd9b1ddc9",
   "metadata": {},
   "source": [
    "## Saving and Loading a Top2Vec Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f69d1f-0a31-4ed6-aac6-461edfa56e65",
   "metadata": {},
   "source": [
    "Once we have a model that we are happy with, we can save it using the `save()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d6563486-02a8-4206-9db7-fc6b4d3a0d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ngram.save(\"../data/top2vec_ngram\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06c6e57-4722-4bd5-bfa0-623233064199",
   "metadata": {},
   "source": [
    "Likewise, we can load up our previous model with `load()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f0f9b071-7330-4fe7-a463-2c77d426d288",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = Top2Vec.load(\"../data/top2vec_ngram\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e7ffdf-fbf5-46d3-a8f2-8eadc15dc9b3",
   "metadata": {},
   "source": [
    "And for good measure we can run the same code as above to print off our bigrams once again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "04b66622-c323-44d9-a56d-3fe3cd0c7630",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1\n",
      "island 0.7406236\n",
      "imprisonment 0.7383562\n",
      "years 0.7350636\n",
      "robben 0.73397404\n",
      "half years 0.7247492\n",
      "years imprisonment 0.7206887\n",
      "sentenced 0.6986825\n",
      "served 0.695978\n",
      "arrested 0.66353947\n",
      "charged 0.66336066\n",
      "banned 0.66275764\n",
      "imprisoned 0.6580816\n",
      "five years 0.6498868\n",
      "suspended sentence 0.6363463\n",
      "prison sentence 0.6340719\n",
      "trial 0.6326254\n",
      "sentence 0.62952983\n",
      "activist 0.6273678\n",
      "months 0.62542427\n",
      "detained 0.6249351\n",
      "prison 0.6236554\n",
      "until 0.6223862\n",
      "poqo 0.6147149\n",
      "custody 0.6120131\n",
      "year sentence 0.6114016\n",
      "held 0.6040434\n",
      "worcester 0.60011864\n",
      "pac 0.5935757\n",
      "released 0.5893775\n",
      "poqo activities 0.58773917\n",
      "charges 0.5808439\n",
      "zwelethemba worcester 0.57607645\n",
      "zweletemba worcester 0.575092\n",
      "arrest 0.5745207\n",
      "robben island 0.571052\n",
      "under 0.56998795\n",
      "worcester cape 0.56770444\n",
      "detention 0.5670366\n",
      "with public 0.56048626\n",
      "ashton 0.55939454\n",
      "later acquitted 0.5478358\n",
      "tortured 0.5445701\n",
      "charged with 0.54265773\n",
      "act 0.5392955\n",
      "interrogation 0.5369543\n",
      "without 0.53288996\n",
      "paarl 0.53190106\n",
      "public 0.5305038\n",
      "prison paarl 0.5296649\n",
      "year prison 0.5292457\n"
     ]
    }
   ],
   "source": [
    "topic_words_ngram, word_scores_ngram, topics_ngram = new_model.get_topics(2)\n",
    "for words, scores, num in zip(topic_words_ngram[1:], word_scores_ngram[1:], topics_ngram[1:]):\n",
    "    print(f\"Topic {num}\")\n",
    "    for word, score in zip(words, scores):\n",
    "        print(word, score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb9ebe1-1f2c-460b-8fb1-96889314d615",
   "metadata": {},
   "source": [
    "If you are working with topic modeling or just want to get a general sense of how your documents align in a large corpus, then Top2Vec is a great solution to your situation. It offers a lot of features for a single line of code. There are other options available that do similar things, notable BerTopic. One should never rely on a single topic modeling approach, nor should one rest arguments heavily upon the results. Topic modeling, regardless of method, is imperfect. It is, however, a great way to explore your data and begin finding patterns that you may otherwise not notice."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
